# Research Ledger — Audio-Reactive LED Project
#
# Permanent record of all findings, experiments, intuitions, and dead ends.
# See LEDGER_GUIDE.md for format details and how to add entries.
#
# Status: spark | exploring | validated | resonates | integrated | dormant | superseded
# Warmth: high | medium | low  (artistic pull, separate from confidence)
# Confidence: high | medium | low  (technical certainty)

entries:

  # ── Feature Extraction ───────────────────────────────────────────

  - id: derivatives-over-absolutes
    date: 2026-01-15
    touched: 2026-02-10
    title: Rate-of-change matters more than absolute values
    summary: >
      Build vs climax has identical static features (RMS ±0.5%) but climax
      brightens 58x faster. The derivative is the signal, not position.
    status: validated
    warmth: high
    confidence: high
    source: research/analysis/taste/build_vs_climax.md
    tags: [feature-extraction, derivatives, brightness]
    relates_to: [build-taxonomy]
    notes: >
      First major insight. Changed thinking about all feature extraction.
      Validated on fa_br_drop1.wav. Needs testing on more songs.

  - id: airiness-context-deviation
    date: 2026-01-18
    touched: 2026-02-10
    title: "Airiness = deviation from local context"
    summary: >
      Two acoustically opposite moments both feel "airy" because both deviate
      from surrounding music's norm. Use deviation-from-running-average,
      not fixed thresholds.
    status: validated
    warmth: high
    confidence: high
    source: research/analysis/taste/air_feature_analysis.md
    tags: [feature-extraction, airiness, context, feelings]
    relates_to: [feeling-layer-human-loop]
    notes: >
      Key principle: feelings are relative to context, not absolute acoustic
      properties. Generalizes beyond airiness to other subjective qualities.

  - id: taps-track-bass-peaks
    date: 2026-01-20
    touched: 2026-02-15
    title: User taps track bass peaks (19ms median), not library onsets
    summary: >
      Only 48.5% of user taps align with librosa onsets. Users switch tracking
      modes across sections (centroid in intro, flux in groove).
    status: validated
    warmth: medium
    confidence: high
    source: research/analysis/taste/beat_tap_analysis.md
    tags: [beat-detection, taps, onsets, bass, user-data]
    relates_to: [tactus-ambiguity, onset-vs-flux, flourish-ratio]
    notes: >
      Implication: library onset detection may be low-value for LED triggering.
      User perception of "beat" is more complex than any single feature.

  - id: build-taxonomy
    date: 2026-01-22
    touched: 2026-02-10
    title: "Build phases: primer, sustained, bridge, drop"
    summary: >
      Four independent build phases. Bridge has HIGHER RMS than drop.
      Drop is sustained intensity, not peak intensity.
    status: validated
    warmth: high
    confidence: high
    source: research/analysis/taste/fa_br_drop1_analysis.md
    tags: [feature-extraction, builds, song-structure, energy]
    relates_to: [derivatives-over-absolutes]
    notes: >
      Primer = cyclic ramps, sustained = upward trajectory, bridge = chaos,
      drop = sustained plateau. All phases independent, not sequential.
      Validated on fa_br_drop1.wav.

  # ── Working Theories ─────────────────────────────────────────────

  - id: flourish-ratio
    date: 2026-01-25
    touched: 2026-02-10
    title: Flourish ratio as section-type selector
    summary: >
      Off-grid vs on-grid tap ratio correlates with section type:
      Ambient (>70%), Accent (30-70%), Groove (<30%).
    status: exploring
    warmth: medium
    confidence: medium
    source: research/analysis/taste/beat_vs_consistent.md
    tags: [beat-detection, flourish, section-detection, user-data]
    relates_to: [taps-track-bass-peaks, flourish-audio-properties]
    notes: >
      Computed from user taps, never from audio alone. Audio-only version
      unbuilt. Could be powerful mode selector if audio-derivable.

  - id: flourish-audio-properties
    date: 2026-01-27
    touched: 2026-02-10
    title: Flourishes are quieter than on-beat taps
    summary: >
      Percussive energy -24.7%, RMS -20.2% for flourishes vs on-beat.
      Effect nearly vanishes for high-confidence flourishes (N=19).
    status: exploring
    warmth: low
    confidence: low
    source: research/analysis/taste/flourish_audio_properties.md
    tags: [beat-detection, flourish, energy, user-data]
    relates_to: [flourish-ratio]
    notes: >
      Effect size drops from -0.533 to -0.007 for high-confidence flourishes.
      May not be robust. Small N warning.

  - id: feeling-layer-human-loop
    date: 2026-01-28
    touched: 2026-02-10
    title: Feeling layer needs human-in-the-loop
    summary: >
      No library provides detect_airiness(). Features exist (centroid,
      flatness, RMS, HPSS) but mapping to feelings is subjective.
    status: resonates
    warmth: high
    confidence: medium
    source: []
    tags: [feelings, architecture, human-loop, art]
    relates_to: [airiness-context-deviation, two-quality-axes]
    notes: >
      Theory, not directly actionable yet. But this IS the core thesis —
      capturing feeling, not just volume. The whole point of the project.

  # ── Tactus & Rhythm ──────────────────────────────────────────────

  - id: tactus-ambiguity
    date: 2026-02-05
    touched: 2026-02-15
    title: Users track different metrical layers per song
    summary: >
      Across 7 Harmonix tracks, taps match correct tempo but phase-shift
      100-250ms from metric grid. Phase varies by track (0.28-0.84).
    status: validated
    warmth: high
    confidence: high
    source: research/analysis/taste/
    tags: [tactus, beat-detection, rhythm, perception, user-data]
    relates_to: [headbangs-half-notes, led-interaction-model, taps-track-bass-peaks]
    notes: >
      Validated across 7 tracks. Lit: Martens 2011 "The Ambiguous Tactus",
      London 2004 "Hearing in Time". User locks onto kick, snare, or off-beat
      depending on the song. Not genre-dependent.

  - id: headbangs-half-notes
    date: 2026-02-08
    touched: 2026-02-15
    title: "Headbangs = half notes (physical constraint)"
    summary: >
      Physical constraint forces lower metrical level (~87 BPM on 180 BPM
      track). Tapped on head-down motion. Every other finger tap.
    status: validated
    warmth: medium
    confidence: high
    source: research/analysis/taste/
    tags: [tactus, rhythm, embodiment, perception]
    relates_to: [tactus-ambiguity]
    notes: >
      Interesting for LED interaction — if audience headbangs at half
      tempo, LEDs pulsing at full tempo might feel "too fast."

  - id: led-interaction-model
    date: 2026-02-08
    touched: 2026-02-08
    title: "LED interaction: Lead vs Follow modes"
    summary: >
      Two modes: (a) Lead — LEDs flash chosen layer to entrain audience tactus.
      (b) Follow — audience taps felt beat, LEDs adapt. Could combine.
    status: spark
    warmth: high
    confidence: low
    source: []
    tags: [interaction, tactus, led-control, future]
    relates_to: [tactus-ambiguity, headbangs-half-notes]
    notes: >
      Future direction. Depends on solving real-time beat tracking first.
      The concept of LEDs as tactus entrainment is exciting.

  # ── Beat Detection ───────────────────────────────────────────────

  - id: beat-trackers-fail-dense-rock
    date: 2026-01-20
    touched: 2026-02-10
    title: All tested beat trackers fail on dense rock
    summary: >
      Tested on Opiate (Tool), best F1=0.500. librosa beat_track doubles
      tempo on syncopated rock (161.5 vs ~83 BPM).
    status: exploring
    warmth: low
    confidence: low
    source: research/analysis/taste/beat_tap_analysis.md
    tags: [beat-detection, rock, failure-mode]
    relates_to: [onset-vs-flux, taps-track-bass-peaks]
    notes: >
      Only tested on 1 song. User hasn't personally verified algorithm
      outputs against the music. May be annotation problem, not algorithm.

  - id: onset-vs-flux
    date: 2026-01-25
    touched: 2026-02-10
    title: "Onset detector ~60% better than bass flux (flawed comparison)"
    summary: >
      Measured against DIFFERENT ground truths (user taps vs Harmonix).
      Not an apples-to-apples comparison.
    status: dormant
    warmth: low
    confidence: low
    source: research/analysis/taste/beat_tap_analysis.md
    tags: [beat-detection, onsets, methodology]
    relates_to: [beat-trackers-fail-dense-rock, bass-flux-electronic-failure]
    notes: >
      The comparison itself is flawed. Need to test both methods against
      the SAME ground truth before drawing conclusions.

  - id: bass-flux-electronic-failure
    date: 2026-01-20
    touched: 2026-01-20
    title: Bass spectral flux fails on continuous sub-bass
    summary: >
      Electronic music with sustained sub-bass gives F1=0.06.
      Self-normalization destroys absolute energy signal.
    status: dormant
    warmth: low
    confidence: high
    source: research/analysis/taste/beat_tap_analysis.md
    tags: [beat-detection, bass, electronic, failure-mode]
    relates_to: [onset-vs-flux, hpss-for-realtime]
    notes: >
      Not a dead end — tells us what bass detection SHOULD be. Fix might
      be HPSS percussive channel. Revisit for real-time pipeline.

  # ── Source Separation ────────────────────────────────────────────

  - id: demucs-offline
    date: 2026-01-10
    touched: 2026-02-10
    title: "Demucs (htdemucs): 4-stem offline separation"
    summary: >
      Drums/bass/vocals/other. ~25s CPU for 50s track. Good quality,
      too slow for real-time.
    status: integrated
    warmth: medium
    confidence: high
    source: research/audio-segments/separated/htdemucs/
    tags: [source-separation, demucs, offline]
    relates_to: [hpss-for-realtime, hs-tasnet-future]
    notes: >
      Already integrated into analysis pipeline. Cached separations
      available for all test tracks.

  - id: hpss-for-realtime
    date: 2026-01-15
    touched: 2026-02-15
    title: "HPSS: 2-stem real-time separation (harmonic/percussive)"
    summary: >
      librosa HPSS, no ML, frame-by-frame. Trivially real-time on ESP32.
      Promising for algorithm work.
    status: integrated
    warmth: high
    confidence: high
    source: audio-reactive/tools/segment.py
    tags: [source-separation, hpss, real-time, esp32]
    relates_to: [demucs-offline, hs-tasnet-future, band-zone-pulse-effect, hpss-vs-flux-empirical]
    notes: >
      Used in band_zone_pulse for streaming percussive detection.
      Compared against spectral flux (see hpss-vs-flux-empirical):
      temporal median does real work — 30% of events differ, background
      color diverges 38%. Streaming version is temporal-median-only
      (missing frequency-axis median) but added complexity is justified.

  - id: hs-tasnet-future
    date: 2026-02-01
    touched: 2026-02-10
    title: "HS-TasNet: potential real-time 4-stem at 23ms latency"
    summary: >
      Exactly our latency budget. No pretrained weights publicly available
      (L-Acoustics proprietary). Would need training on MusDB (~86 tracks).
    status: dormant
    warmth: medium
    confidence: low
    source: []
    tags: [source-separation, real-time, ml, future]
    relates_to: [hpss-for-realtime, demucs-offline]
    notes: >
      Future investment if HPSS proves insufficient. pip install hs-tasnet
      (lucidrains). Training infrastructure not set up.

  - id: separation-dead-ends
    date: 2026-02-01
    touched: 2026-02-01
    title: "Dead ends: DTTNet, RT-STT, Spleeter, GPU Audio SDK"
    summary: >
      DTTNet: broken weight links, 4 models, CPU too slow. RT-STT: no code.
      Spleeter: dated quality. GPU Audio SDK: C++ only.
    status: superseded
    warmth: low
    confidence: high
    source: []
    tags: [source-separation, dead-end]
    relates_to: [demucs-offline, hpss-for-realtime]
    notes: >
      Investigated Feb 2026. None viable for our use case. Landscape may
      change — new models release frequently.

  # ── WLED Sound Reactive ──────────────────────────────────────────

  - id: wled-sr-reimplemented
    date: 2026-02-01
    touched: 2026-02-15
    title: WLED-SR algorithms reimplemented in Python
    summary: >
      Key finding: WLED beat detection is a simple bin threshold. No spectral
      flux, no tempo tracking, no feeling layer. Their magic is in visual
      effects, not audio analysis.
    status: validated
    warmth: medium
    confidence: high
    source: audio-reactive/effects/wled_sr/
    tags: [wled, external, beat-detection, effects, pillar-2]
    relates_to: [wled-vs-custom-untested, two-quality-axes]
    notes: >
      Python reimplementation enables A/B testing framework. WLED effects
      surprisingly effective despite simple audio analysis. Lesson:
      visual design matters as much as audio analysis quality.

  - id: wled-vs-custom-untested
    date: 2026-02-01
    touched: 2026-02-15
    title: "Custom vs WLED-SR: untested on actual LEDs"
    summary: >
      Need side-by-side LED test to know if our approach is actually
      better. Theory says yes, but theory isn't proof.
    status: exploring
    warmth: medium
    confidence: low
    source: []
    tags: [wled, comparison, validation, pillar-2]
    relates_to: [wled-sr-reimplemented]
    notes: >
      Key validation milestone. Until tested on hardware, "our approach
      is better" remains an unvalidated claim.

  # ── Effects & Visual Design ──────────────────────────────────────

  - id: band-zone-pulse-effect
    date: 2026-02-10
    touched: 2026-02-18
    title: "Band Zone Pulse: frequency-zoned percussive sparkles"
    summary: >
      5 frequency zones mapped to LED strip positions. Streaming HPSS
      separates percussive hits. Per-band peak detection with cooldowns.
    status: integrated
    warmth: high
    confidence: high
    source: audio-reactive/effects/band_zone_pulse.py
    tags: [effects, hpss, frequency-bands, sparkles]
    relates_to: [hpss-for-realtime, diamond-topology, hue-shift-low-brightness, centroid-position-clustering]
    notes: >
      Running on diamond sculpture. Zone colors tuned for COB strip.
      Background glow disabled for testing. Zone positions drift
      slowly (~10s) for visual freshness. Key design decisions:
      (1) Dominant band gate >15% total percussive energy so dominant
      instrument visually dominates. (2) Same-band re-trigger reuses
      sparkle position, preventing overlap. (3) Random position within
      zone over spectral centroid (see centroid-position-clustering).
      (4) Non-zero color channels clamped to 1.0 during fade to prevent
      hue shift (see hue-shift-low-brightness).

  - id: diamond-topology
    date: 2026-02-15
    touched: 2026-02-18
    title: "Diamond sculpture: 3-branch height-mapped topology"
    summary: >
      73 physical LEDs (42 up + 20 down + 11 up). Height mode maps logical
      LED array across branches sharing a common height axis.
    status: integrated
    warmth: high
    confidence: high
    source: audio-reactive/hardware/sculptures.json
    tags: [hardware, topology, sculpture, mapping]
    relates_to: [band-zone-pulse-effect, gamma-height-curve]
    notes: >
      Branch 3 (up2) uses gamma=0.55 for physical height alignment.
      Compensates for geometry where branch rises steeply then levels off.

  - id: gamma-height-curve
    date: 2026-02-18
    touched: 2026-02-18
    title: "Gamma curve for non-linear branch height mapping"
    summary: >
      Added per-branch gamma parameter to sculpture topology. gamma<1
      makes branch rise fast at base, slow at top. up2 uses 0.55 to
      shift zone boundaries down ~2 LEDs for visual alignment.
    status: integrated
    warmth: high
    confidence: high
    source: audio-reactive/hardware/sculptures.json
    tags: [hardware, topology, sculpture, mapping]
    relates_to: [diamond-topology, band-zone-pulse-effect]
    notes: >
      Applied in runner.py apply_topology(). Default gamma=1.0 (linear).
      Adjustable per-branch in sculptures.json.

  - id: three-voices-effect
    date: 2026-01-28
    touched: 2026-02-10
    title: "Three Voices: multi-layer audio decomposition effect"
    summary: >
      Early effect exploring harmonic/percussive/transient layering
      as separate visual voices on the LED strip.
    status: integrated
    warmth: medium
    confidence: medium
    source: audio-reactive/effects/three_voices.py
    tags: [effects, hpss, decomposition]
    relates_to: [hpss-for-realtime]
    notes: >
      One of the first effects built. May need revisiting with newer
      understanding of feature extraction.

  # ── Architecture & Decisions ─────────────────────────────────────

  - id: nebula-pipeline-foundation
    date: 2026-01-05
    touched: 2026-02-10
    title: "Architecture: build on nebula streaming pipeline"
    summary: >
      Python audio callback stores state, fixed-rate main loop reads state
      and does serial flush. Decoupled by WS2812B timing requirement.
    status: integrated
    warmth: high
    confidence: high
    source: audio-reactive/effects/runner.py
    tags: [architecture, pipeline, serial, hardware]
    relates_to: []
    notes: >
      Non-negotiable constraint from WS2812B timing. This IS the architecture.
      All effects must respect this callback/render split.

  - id: two-quality-axes
    date: 2026-01-15
    touched: 2026-02-10
    title: "Two independent quality axes: decomposition AND mapping"
    summary: >
      Audio decomposition quality (can we extract features?) is separate
      from LED mapping quality (do the LEDs look/feel right?). Both matter.
    status: resonates
    warmth: high
    confidence: high
    source: []
    tags: [architecture, quality, philosophy]
    relates_to: [feeling-layer-human-loop, wled-sr-reimplemented]
    notes: >
      Perfect audio analysis with bad LED mapping = bad. Crude audio with
      great LED mapping = might be great. WLED proves this — simple
      analysis, effective visuals.

  - id: pillar-separation
    date: 2026-01-15
    touched: 2026-02-10
    title: "Keep internal research separate from external (WLED etc)"
    summary: >
      Pillar 1 = our own feature extraction research. Pillar 2 = external
      algorithms (WLED-SR etc). May compose later, keep separate for now.
    status: integrated
    warmth: medium
    confidence: high
    source: []
    tags: [architecture, organization, pillar-1, pillar-2]
    relates_to: [wled-sr-reimplemented]
    notes: >
      Prevents premature coupling. Each pillar can evolve independently.

  # ── Data & Methodology ───────────────────────────────────────────

  - id: data-assets
    date: 2026-01-10
    touched: 2026-02-15
    title: Audio test data and annotation inventory
    summary: >
      Opiate Intro (40s, 6 annotation layers), fa_br_drop1 (129s, sections),
      ambient (30s), electronic_beat (50s), 7 Harmonix tracks with tap annotations.
    status: integrated
    warmth: medium
    confidence: high
    source: research/audio-segments/catalog.yaml
    tags: [data, annotations, user-data]
    relates_to: [taps-track-bass-peaks, tactus-ambiguity]
    notes: >
      All Harmonix tracks have sethbeat tap annotations. Constant Motion
      also has headbang layer. See catalog.yaml for full details.

  - id: keyboard-repeat-artifacts
    date: 2026-01-20
    touched: 2026-01-20
    title: 5 keyboard repeat artifacts in Opiate taps
    summary: >
      At 29.3-29.6s, 5 taps at 83-85ms intervals = keyboard auto-repeat,
      not intentional taps. Data quality issue.
    status: validated
    warmth: low
    confidence: high
    source: research/audio-segments/catalog.yaml
    tags: [data-quality, taps, artifacts]
    relates_to: [taps-track-bass-peaks]
    notes: >
      Need to filter from analysis. Known positions: 29.3-29.6s in Opiate.

  - id: ml-project-framing
    date: 2026-01-15
    touched: 2026-02-10
    title: "Project framed as ML: training/validation/test split"
    summary: >
      Conversations + analysis = training. Decomposition quality = validation.
      User taps = automated test (cheap, possibly wrong). Looking at LEDs =
      manual test (ground truth, expensive).
    status: integrated
    warmth: medium
    confidence: high
    source: []
    tags: [methodology, philosophy]
    relates_to: [two-quality-axes]
    notes: >
      User tap data is test set, not guaranteed truth. User could be wrong
      or miscommunicate. Always validate with LED visual test when possible.

  # ── Future Directions ────────────────────────────────────────────

  - id: client-side-web-viewer
    date: 2026-02-01
    touched: 2026-02-10
    title: Pure JS web viewer for GitHub Pages
    summary: >
      Rewrite segment.py web as Web Audio API + Canvas. Shareable demo
      with zero install.
    status: spark
    warmth: medium
    confidence: high
    source: []
    tags: [tools, web, sharing, future]
    relates_to: [repo-discoverability]
    notes: >
      Significant project, plan first. Blocked by having something
      worth showing publicly.

  - id: repo-discoverability
    date: 2026-02-01
    touched: 2026-02-10
    title: "Repo rename + README + community post"
    summary: >
      Rename from 'led', set GitHub topics, rewrite README with findings,
      post to r/FastLED or r/WLED.
    status: spark
    warmth: low
    confidence: high
    source: []
    tags: [community, sharing, future]
    relates_to: [client-side-web-viewer]
    notes: >
      Blocked until repo is ready to share. Need polished demo first.

  - id: hetzner-gastown
    date: 2026-02-10
    touched: 2026-02-10
    title: Hetzner cloud server for multi-agent development
    summary: >
      CPX11 (4.51 EUR/month). Enables voice-to-code workflow while traveling.
    status: spark
    warmth: low
    confidence: high
    source: research/docs/HETZNER_SETUP.md
    tags: [infrastructure, development, future]
    relates_to: []
    notes: >
      Guide and setup script already written. Just needs execution.

  # ── Feb 18 session merges ────────────────────────────────────────

  - id: hpss-vs-flux-empirical
    date: 2026-02-18
    touched: 2026-02-18
    title: "HPSS vs spectral flux: ~30% disagreement, different detectors"
    summary: >
      Compared streaming HPSS against per-band spectral flux on 8 tracks.
      Only 70% of events match. Flux is chattier — triggers on harmonic
      onsets (chords, vocals) that HPSS suppresses. Background color
      diverges 38% on average.
    status: validated
    warmth: medium
    confidence: high
    source: audio-reactive/research/analysis/compare_hpss_vs_flux.py
    tags: [source-separation, hpss, feature-extraction, experiments]
    relates_to: [hpss-for-realtime, band-zone-pulse-effect, flux-4frame-avg]
    notes: >
      Initial hypothesis was flux would simplify HPSS. Disproven — they're
      genuinely different detectors. The 847 "extra" flux events are real
      energy increases the temporal median absorbs. Whether they make better
      or worse sparkles is a visual question, not a numerical one.

  - id: flux-4frame-avg
    date: 2026-02-18
    touched: 2026-02-18
    title: "4-frame average flux: continuum between naive flux and HPSS"
    summary: >
      Comparing against 4-frame running average (~46ms) suppresses gradual
      harmonic changes. HPSS recall 77% to 83%. Regression on dense
      percussive material (amen_break 95% to 79%).
    status: exploring
    warmth: medium
    confidence: high
    source: audio-reactive/effects/band_sparkle_flux.py
    tags: [source-separation, feature-extraction, experiments]
    relates_to: [hpss-vs-flux-empirical, hpss-for-realtime]
    notes: >
      The continuum: flux(1-frame) → flux(N-frame avg) → HPSS(N-frame median).
      More context = more selective = fewer harmonic false positives but also
      fewer "musically interesting non-percussive" triggers. Dense percussive
      material favors 1-frame. Created band_sparkle_flux effect for visual
      A/B testing. Awaiting evaluation on LEDs.

  - id: nmf-not-suited-for-sparkles
    date: 2026-02-18
    touched: 2026-02-18
    title: "NMF: overkill for band-level transient detection"
    summary: >
      NMF solves source separation (what instrument) but the effect only
      needs band-level transient detection (did energy spike). More expensive,
      less stable in streaming, no clear visual benefit.
    status: dormant
    warmth: low
    confidence: high
    source: []
    tags: [source-separation, dead-end]
    relates_to: [hpss-vs-flux-empirical, hpss-for-realtime]
    notes: >
      Existing NMF separations in audio-segments/separated/nmf/ useful
      for offline analysis but not for real-time effects.

  - id: hue-shift-low-brightness
    date: 2026-02-18
    touched: 2026-02-18
    title: WS2812B hue shift at low brightness — smaller channels drop first
    summary: >
      Colors with unequal channel ratios (e.g. orange R255 G140 B0) shift
      hue during fade because the smaller channel rounds to 0 in uint8
      before the dominant. Orange appears red at tail end of decay.
    status: validated
    warmth: medium
    confidence: high
    source: audio-reactive/effects/band_zone_pulse.py
    tags: [hardware, effects, failure-mode]
    relates_to: [band-zone-pulse-effect]
    notes: >
      Fix: clamp non-zero color channels to minimum 1.0 during compositing,
      use higher brightness cutoff (0.04). General issue affecting any
      effect using multi-channel colors at low brightness on WS2812B.

  - id: centroid-position-clustering
    date: 2026-02-18
    touched: 2026-02-18
    title: Spectral centroid clusters mid-zone — tree feels underutilized
    summary: >
      Mapping sparkle position to spectral centroid within a band results
      in positions clustering around center. Even with adaptive normalization,
      observed centroid range is much narrower than the band width.
    status: dormant
    warmth: low
    confidence: high
    source: audio-reactive/effects/band_zone_pulse.py
    tags: [effects, feature-extraction, dead-end]
    relates_to: [band-zone-pulse-effect]
    notes: >
      Reverted to random placement. Technically correct but visually
      unsatisfying — the tree never uses its full height. Random with
      same-instrument re-trigger gives better spatial coverage. Could
      revisit with per-instrument tracking (not per-band).

  - id: hpss-redundancy-with-threshold
    date: 2026-02-18
    touched: 2026-02-18
    title: HPSS may be redundant when threshold-based detection is used
    summary: >
      The mean+2.5*std threshold already filters transients. HPSS's main
      value is cleaning percussive signal when sustained tonal content
      overlaps the same band as a hit. For percussive-only mode, raw
      spectrum + threshold works ~90% as well.
    status: exploring
    warmth: low
    confidence: medium
    source: audio-reactive/effects/band_zone_pulse.py
    tags: [source-separation, architecture]
    relates_to: [band-zone-pulse-effect, hpss-vs-flux-empirical]
    notes: >
      HPSS becomes essential if harmonic background glow is re-enabled
      (needs clean harmonic-only energy). For percussive-only display,
      mostly overhead. Kept because cost is modest and it does help
      in dense mixes.

  - id: rolling-integral-sustained-energy
    date: 2026-02-18
    touched: 2026-02-18
    title: 5s rolling integral as sustained energy detector (untested)
    summary: >
      Rolling sum of RT-normalized energy over 5s window per band.
      Distinguishes sustained elevation from transient spikes.
    status: exploring
    warmth: medium
    confidence: medium
    source: audio-reactive/tools/viewer.py
    tags: [feature-extraction, effects]
    relates_to: [build-taxonomy, derivatives-over-absolutes]
    notes: >
      Visualization tool only — untested as LED mapping input. Hypothesis:
      integral could distinguish drops (sustained high) from builds (rising)
      from breakdowns (falling). Needs validation against fa_br_drop1.

  - id: hpss-harmonic-filters-vocals
    date: 2026-02-19
    touched: 2026-02-19
    title: HPSS harmonic component strips vocal energy from mids
    summary: >
      Rap vocals are heavily percussive (consonants, plosives). HPSS classifies
      most vocal energy as percussive, leaving harmonic mids empty. Full spectrum
      matches human perception better for "overall feel" questions.
    status: validated
    warmth: medium
    confidence: high
    source: audio-reactive/effects/band_zone_pulse.py
    tags: [source-separation, feature-extraction, effects]
    relates_to: [hpss-for-realtime, hpss-redundancy-with-threshold]
    notes: >
      Design rule: use HPSS only where you specifically need it (percussive
      hit detection). For "what does the music sound like overall," full
      spectrum matches perception better.

  - id: vote-counting-bg-color
    date: 2026-02-19
    touched: 2026-02-19
    title: Vote counting beats energy integral for background color selection
    summary: >
      Rolling energy integral was dominated by bass (high energy = highest
      integral). Vote counting: each frame votes for winning group, count
      over 5s. Bass gets one vote per frame — consistency gives no extra credit.
    status: integrated
    warmth: medium
    confidence: medium
    source: audio-reactive/effects/band_zone_pulse.py
    tags: [effects, architecture]
    relates_to: [band-zone-pulse-effect]
    notes: >
      Instantaneous switching blended rapidly changing targets into white
      via the ~2-3s crossfade. Vote approach is the right middle ground.

  - id: accent-vs-groove-effects
    date: 2026-02-19
    touched: 2026-02-19
    title: "Effect taxonomy: accent effects vs groove effects"
    summary: >
      Band zone pulse is an accent effect — highlights individual hits. Works
      for reggae/electronic (sparse hits) but not rap (dense percussive vocals,
      groove feel). Rap needs groove effects — tempo-locked pulsing, not per-hit.
    status: resonates
    warmth: high
    confidence: medium
    source: []
    tags: [effects, feelings, architecture, future]
    relates_to: [flourish-ratio, feeling-layer-human-loop, two-quality-axes, band-zone-pulse-effect]
    notes: >
      Connects to flourish ratio: groove sections have <30% off-grid taps,
      meaning locked regular beat. Accent effects fight that feel. The
      beat predictor in signals.py may be better for groove genres. Also
      connects to two quality axes — decomposition is fine for rap, it's
      the visual mapping that doesn't suit the genre.

  - id: pulse-compositing-replace-not-blend
    date: 2026-02-19
    touched: 2026-02-19
    title: "LED compositing: pulses must replace background, not blend"
    summary: >
      Additive compositing shifts pulse hue. Per-channel max lets wrong
      channels win. Correct: pulse replaces background at its pixels,
      fades to black, then background fades back via per-pixel opacity.
    status: integrated
    warmth: medium
    confidence: high
    source: audio-reactive/effects/band_zone_pulse.py
    tags: [effects, architecture, hardware]
    relates_to: [hue-shift-low-brightness, band-zone-pulse-effect]
    notes: >
      Three strategies tried: additive (shifts hue), per-channel max
      (wrong channels win), replace + fade-in (correct). Fade-in uses
      per-pixel opacity that drops to 0 on pulse, recovers over ~0.5s.

  - id: absint-fails-percussive
    date: 2026-02-19
    touched: 2026-02-19
    title: Abs-integral autocorrelation fails on percussive music
    summary: >
      Abs-integral merges onset and offset energy, destroying timing info.
      Autocorrelation locks to wrong periodicities (0/4 rap tracks, 2/3
      ratio error on complex beats). Good for beat detection, bad for tempo.
    status: validated
    warmth: low
    confidence: high
    source: audio-reactive/research/analysis/scripts/rap_tempo_analysis.py
    tags: [beat-detection, feature-extraction, dead-end]
    relates_to: [derivatives-over-absolutes, onset-envelope-for-tempo]
    notes: >
      Symmetric bump-dip pattern creates spurious periodicities at
      non-octave ratios. Keep absint for beat detection in rock/metal.
      Tempo estimation needs a different signal.

  - id: onset-envelope-for-tempo
    date: 2026-02-19
    touched: 2026-02-19
    title: Multi-band onset envelope is the right signal for tempo autocorrelation
    summary: >
      FFT → 6 mel bands → log energy → diff → half-wave rectify → mean.
      Without tempo prior, autocorrelation consistently finds clean power-of-2
      multiples of true beat. Multi-band matters — single-band misses spectral
      decomposition.
    status: integrated
    warmth: high
    confidence: high
    source: audio-reactive/effects/signals.py
    tags: [beat-detection, feature-extraction, effects]
    relates_to: [absint-fails-percussive, no-prior-for-leds]
    notes: >
      6 mel bands is enough — tested against 128, same autocorrelation
      peak structure. Implemented as OnsetTempoTracker in signals.py.

  - id: no-prior-for-leds
    date: 2026-02-19
    touched: 2026-02-19
    title: "Tempo prior hurts LEDs — clean octaves beat exact BPM"
    summary: >
      Without prior: 0/9 exact BPM but 9/9 clean octave multiples.
      With prior: some exact but introduces 2/3, 4/3 ratio errors.
      For LED fades any octave is fine — no prior is the correct default.
    status: validated
    warmth: high
    confidence: high
    source: audio-reactive/research/analysis/scripts/test_onset_tracker.py
    tags: [beat-detection, effects, perception]
    relates_to: [onset-envelope-for-tempo, two-quality-axes]
    notes: >
      Case of two-quality-axes: decomposition accuracy ≠ LED quality.
      Consistent power-of-2 errors strictly better than unpredictable
      ratio errors for visual effects.

  - id: ac-buffer-must-fill
    date: 2026-02-19
    touched: 2026-02-19
    title: Autocorrelation on partial buffer permanently poisons tempo estimate
    summary: >
      Running autocorrelation before ring buffer is full produces garbage.
      The 80/20 period smoother then rejects correct tempo forever because
      correct/garbage ratio falls outside correction windows. Must wait for
      full buffer (~5s) before first estimate.
    status: integrated
    warmth: low
    confidence: high
    source: audio-reactive/effects/signals.py
    tags: [beat-detection, failure-mode]
    relates_to: [onset-envelope-for-tempo]
    notes: >
      Non-obvious failure: smoother's rejection window means first-estimate
      quality is critical. One bad initial value locks out the correct one.

  - id: openviking-analysis
    date: 2026-02-19
    touched: 2026-02-19
    title: "OpenViking source analysis: prompt techniques worth stealing"
    summary: >
      Read OpenViking source code (volcengine/OpenViking). Their memory system
      is pure prompt engineering, no ML. Novel techniques: L0/L1/L2 layered
      abstracts, negative extraction examples, CREATE/MERGE/SKIP dedup, and
      bottom-up category re-summarization. "Self-evolving" claim is marketing.
    status: validated
    warmth: medium
    confidence: high
    source: []
    tags: [architecture, methodology, tools, external]
    relates_to: [research-ledger-system]
    notes: >
      Actionable takeaways for our ledger:
      (1) Add abstract field — one-line grep target per entry.
      (2) Negative extraction examples — the reason agents logged bug
      fixes is nobody said "don't log implementation details." Our guide
      now handles this via "What Belongs in the Ledger" section.
      (3) CREATE/MERGE/SKIP — make explicit before adding: is this new,
      should it update existing, or is it a duplicate? Findings/decisions
      are immutable; theories/entities are mergeable.
      (4) Bottom-up category summaries — script that regenerates per-tag
      summaries when entries change. Future nice-to-have.
      Not worth adopting: vector embeddings, hierarchical retrieval,
      AGFS filesystem, async queues. Our warmth field is more advanced
      than anything they implemented. Their active_count is write-only.

  - id: research-ledger-system
    date: 2026-02-18
    touched: 2026-02-18
    title: "Research ledger: structured YAML for tracking findings across sessions"
    summary: >
      Created a searchable ledger (this file) to replace the growing MEMORY.md.
      Entries have status (spark→integrated), warmth (artistic pull), confidence
      (technical certainty), and relates_to links. Session files enable
      concurrent multi-agent updates.
    status: integrated
    warmth: high
    confidence: high
    source: audio-reactive/research/LEDGER_GUIDE.md
    tags: [architecture, methodology, tools]
    relates_to: []
    notes: >
      Key design choice: warmth and confidence as independent axes.
      High-confidence low-warmth = technically true but not exciting.
      Low-confidence high-warmth = unproven but pulls you toward it.
      Status vocabulary includes "dormant" (hasn't found its place)
      and "resonates" (feels right, not fully proven) to avoid forcing
      binary useful/not-useful judgments on an art project. Rule of
      thumb for what to log: if it changes how we THINK about audio,
      LEDs, or feelings, log it. Implementation details belong in git.

  # ── Prior Art & Hardware Landscape (Feb 20) ─────────────────────

  - id: stm32-vs-esp32
    date: 2026-02-20
    touched: 2026-02-20
    title: "STM32 vs ESP32: industrial workhorse vs IoT all-in-one"
    summary: >
      STM32 = no WiFi/BT, better real-time (DMA+timer flush WS2812 without CPU),
      huge variant family, $2 Blue Pill. ESP32 = WiFi+BT built in, dual-core 240MHz,
      good-enough real-time, $4-8. Ténéré used 208 STM32 Blue Pills + W5500 Ethernet
      because they needed cheap distributed controllers doing one job. ESP32 is right
      for standalone audio-reactive at our scale (197 LEDs, needs WiFi for web viewer).
    status: validated
    warmth: low
    confidence: high
    source: []
    tags: [hardware, architecture, external]
    relates_to: [nebula-pipeline-foundation]
    notes: >
      STM32's DMA can flush WS2812 data without CPU involvement — processor sets up
      transfer and walks away. ESP32's WiFi stack can steal CPU cycles, making real-time
      slightly less deterministic, but irrelevant at 197 LEDs. STM32 becomes relevant
      if scaling beyond ~500 LEDs (distributed branch controllers like Ténéré).

  - id: tenere-architecture
    date: 2026-02-20
    touched: 2026-02-20
    title: "Tree of Ténéré: distributed LED architecture at 175K scale"
    summary: >
      175,000 LEDs, 25,000 leaves, 208 STM32 branch controllers. Laptop runs LX Studio
      (animation engine), sends pixel data over wired Ethernet (OPC/UDP) to branch
      controllers, each flushes its local LEDs via FastLED. Sensors (Muse EEG, pulse,
      mic) on Raspberry Pis send OSC over WiFi to LX Studio. All open source but
      repos now private/empty — only READMEs and external references remain.
    status: validated
    warmth: medium
    confidence: medium
    source: []
    tags: [hardware, architecture, external, prior-art]
    relates_to: [nebula-pipeline-foundation, stm32-vs-esp32, osc-as-sensor-glue]
    notes: >
      Key lesson: at scale, you separate animation computation (laptop) from LED
      flush (microcontrollers). Our architecture does both on one ESP32, which is
      fine at 197 LEDs but wouldn't scale. Sensor integration via OSC is the
      pattern worth stealing — any sensor on any device can drive any visual
      parameter through a common protocol. GitHub repos (treeoftenere/*) are
      private or empty as of 2026-02. External sources (designboom, dezeen,
      lx.studio spotlight) confirm the architecture but implementation details
      are unverifiable. The Interactivity repo (treeoftenere/Interactivity) is
      real code — Muse EEG signal processing, OSC bridges, sensor integration —
      but domain-specific to BCI/biometrics, not providing novel techniques for
      audio-reactive LED work. The signal processing pattern (decompose → extract
      features → normalize → OSC → visuals) is structurally identical to ours
      but the implementation details are EEG-specific and not transferable.

  - id: osc-as-sensor-glue
    date: 2026-02-20
    touched: 2026-02-20
    title: "OSC protocol: universal sensor-to-animation glue"
    summary: >
      Open Sound Control = structured UDP messages with path + values
      (e.g. /muse/calm 0.73). Ténéré used it to connect Muse EEG, pulse sensors,
      and audio to LX Studio. Any sensor that can emit OSC plugs into any
      animation engine that receives it. Lightweight, real-time, well-supported
      in Python (pyliblo).
    status: spark
    warmth: medium
    confidence: high
    source: []
    tags: [architecture, hardware, future]
    relates_to: [tenere-architecture, feeling-layer-human-loop]
    notes: >
      Not needed now (single ESP32, audio only), but if we add sensors
      (MIDI controller for live parameter tweaking, accelerometer for
      movement detection), OSC is the right integration protocol.
      Python mido library also reads MIDI in ~5 lines.

  - id: fluora-pixelair-prior-art
    date: 2026-02-20
    touched: 2026-02-20
    title: "Fluora/PixelAir: polished consumer LED product with no real audio analysis"
    summary: >
      Color+Light (ex-Symmetry Labs, SF) sells Fluora LED houseplants ($250-$500).
      300 LEDs in 3D-mapped polycarbonate leaves, 60 FPS on-device rendering via
      proprietary Photon/PixelAir engine. "Music sync" is basic amplitude/rhythm via
      phone mic over WiFi. No spectral decomposition, no feeling detection. $221K
      Indiegogo validates consumer market for 3D LED art.
    status: validated
    warmth: medium
    confidence: high
    source: []
    tags: [external, prior-art, art]
    relates_to: [two-quality-axes, wled-sr-reimplemented]
    notes: >
      Same lesson as WLED: visual polish > audio sophistication in the market.
      Their attack/decay parameters in the app are worth noting — intuitive UX
      for envelope control. Audio-reactive is a massive gap in their product,
      validating both that it's hard and that it's underserved.
      Open source: OpenPixelControl (github.com/SymmetryLabs/openpixelcontrol)
      uses JSON 3D layout files similar to our sculptures.json.

  - id: daw-concepts-for-leds
    date: 2026-02-20
    touched: 2026-02-20
    title: "DAW concepts worth stealing: sidechain, sends, ADSR, wet/dry"
    summary: >
      Sidechain compression (one audio feature suppresses another visual element)
      creates push-pull tension. Sends (one signal to multiple destinations at
      different amounts) enables rich parameter mapping. ADSR envelopes make
      temporal response tunable per-effect. Wet/dry mix blends audio-reactive
      with ambient baseline, preventing dead LEDs during silence.
    status: spark
    warmth: high
    confidence: high
    source: []
    tags: [architecture, effects, feelings, future]
    relates_to: [two-quality-axes, feeling-layer-human-loop, accent-vs-groove-effects]
    notes: >
      The meta-insight: best DAW engineers create relationships between elements,
      not independent mappings. Audio features should modulate each other's
      influence on visuals. Bass energy modulates how strongly transients affect
      brightness. Spectral flux modulates color propagation speed. Visual elements
      breathe together because they're coupled through audio, not because each
      independently reacts to it. Sidechain is highest priority to prototype.

  - id: edm-drop-detection-paper
    date: 2026-02-20
    touched: 2026-02-21
    title: "Yadati et al. 2014: EDM drop detection via SoundCloud timed comments"
    summary: >
      Only paper specifically targeting EDM structural events (drop/build/break).
      Two-stage approach: model sound during drops, incorporate temporal structure.
      Features: spectrograms, MFCCs, rhythm. F1=0.96 at 15s tolerance. Used
      SoundCloud timed comments as noisy labels. Expanded dataset: 402 CC-licensed
      tracks on OSF (osf.io/eydxk) with ground truth for drops, builds, breaks.
    status: validated
    warmth: medium
    confidence: high
    source:
      - https://archives.ismir.net/ismir2014/paper/000297.pdf
      - https://ieeexplore.ieee.org/document/8279544/
    tags: [external, prior-art, beat-detection, feature-extraction]
    relates_to: [build-taxonomy, rolling-integral-sustained-energy, yadati-edm-drops-audit]
    notes: >
      15s tolerance is very coarse for real-time LED response. Their social-comment
      labeling is clever but won't help for live audio. The 402-track dataset
      (CC BY 4.0) is the only public EDM structure dataset with drop/build/break
      annotations. SoundCloud API no longer accepting new registrations as of
      ~2022, but timed comments still exist and can be scraped unofficially.
      See yadati-edm-drops-audit for full dataset quality analysis.

  - id: yadati-edm-drops-audit
    date: 2026-02-21
    touched: 2026-02-21
    title: "Yadati EDM dataset (402 tracks): independent audit"
    summary: >
      402 CC-licensed EDM tracks with drop/build/break annotations. Labels derived
      from SoundCloud timed comments (inherently noisy) plus manual expert ground truth.
      The novel social-media-as-weak-labels approach is genuinely interesting, but expert
      annotation methodology is under-documented and CC SoundCloud tracks skew amateur.
    status: validated
    warmth: medium
    confidence: high
    source:
      - https://osf.io/eydxk/
      - https://archives.ismir.net/ismir2014/paper/000297.pdf
      - https://ieeexplore.ieee.org/document/8279544/
    tags: [dataset, external, audit, edm, structure]
    relates_to: [edm-drop-detection-paper]
    notes: >
      ANNOTATION METHODOLOGY: Two-layer. (1) Weak labels: SoundCloud timed comments
      containing "drop"/"build"/"break" keywords — ~600 comments per event type.
      (2) Expert ground truth: manual annotations, but who the experts were, how many,
      what training, what inter-annotator agreement is never documented. The 2018 IEEE
      TMM paper expanded from 100 to 402 tracks but expansion documentation is sparse.
      Event counts: 435 drops (1.08/track), 596 builds (1.48/track), 372 breaks (0.92/track).

      REPRODUCIBILITY: Easy. Audio and annotations bundled on OSF, direct download,
      CC-licensed. Download and go — no matching, no external sources, no link rot risk.

      STRENGTHS: CC audio (rare — actually redistributable), addresses under-studied
      EDM events, noisy-label methodology is itself a contribution, three event types,
      ground truth + weak labels useful for studying label noise.

      WEAKNESSES: Expert annotation protocol under-documented. SoundCloud bias toward
      tracks generating social engagement — quiet/experimental EDM underrepresented.
      Selection bias: only tracks WITH drop comments included. CC-licensed SoundCloud
      skews amateur/semi-pro, not mainstream commercial EDM. 402 tracks modest for deep
      learning. Event boundaries inherently fuzzy (15s tolerance in original eval).
      Original paper used only 100 tracks (60/20/20); 402-track expansion poorly documented.

  - id: harmonix-set-audit
    date: 2026-02-21
    touched: 2026-02-21
    title: "Harmonix Set (912 tracks): independent audit"
    summary: >
      Largest beat+downbeat+segment dataset for Western popular music, from the
      Rock Band / Dance Central company. Professional annotators using DAW workflows.
      Major strength is scale and joint beat+segment annotations. Major weakness:
      single annotator per track, zero inter-annotator agreement data, and functional
      segment labels are inconsistent enough to require post-processing.
    status: validated
    warmth: medium
    confidence: high
    source:
      - https://github.com/urinieto/harmonixset
      - https://archives.ismir.net/ismir2019/paper/000068.pdf
    tags: [dataset, external, audit, beats, segments, pop]
    relates_to: [edm-drop-detection-paper, tactus-ambiguity]
    notes: >
      ANNOTATION METHODOLOGY: Musical experts at Harmonix Music Systems using DAW
      software. Process: (1) tempo track, (2) beats/downbeats, (3) segment boundaries
      + functional labels. Constraint: segment boundaries always fall on an annotated
      beat (by design, not observation). Single annotator per track. No inter-annotator
      agreement reported. Unclear if repurposed game assets or purpose-built research
      annotations. 912 tracks: hip hop, dance, rock, metal, Western pop.

      REPRODUCIBILITY: Fragile. Audio not distributed — must fetch via YouTube URLs
      and verify with provided DTW alignment scores. Link rot degrades availability
      year over year. Pre-computed mel spectrograms available as a partial workaround.
      Legality of YouTube downloading is a gray area: violates YouTube ToS (contract,
      not law), but MIR community leans on fair use (non-commercial, short excerpts,
      transformative). The dataset deliberately distributes URLs not audio, shifting
      the legal burden to each researcher. No MIR researcher has been sued for this,
      but that's precedent-by-absence, not clearance.

      STRENGTHS: Largest beat+segment dataset (912 tracks). Joint annotations enable
      multi-task research. Beat-aligned segments useful constraint. Professional
      annotators. Rich metadata (MusicBrainz, genre). MIT license. Pre-computed mel
      spectrograms available.

      WEAKNESSES: Single annotator, no agreement data — significant gap for subjective
      tasks like structure analysis. Functional labels inconsistent: "prechorus",
      "postchorus" exist but no controlled vocabulary documented. Downstream users
      must improvise label merging (see mir-aidj/all-in-one#8), reducing reproducibility.
      Audio not distributed — YouTube URLs rot over time. Genre coverage Western
      pop-centric; EDM, jazz, classical absent.

  - id: salami-dataset-audit
    date: 2026-02-21
    touched: 2026-02-21
    title: "SALAMI (1300+ tracks, 2200+ annotations): independent audit"
    summary: >
      Methodological gold standard for music structure annotation. Two trained
      annotators per track, hierarchical (coarse+fine+functional) annotations,
      published annotator guide, 9 named annotators, broadest genre diversity.
      Critical practical obstacles: no audio distribution, known formatting errors
      in pre-v2.0 versions (many papers trained on corrupt data), and inherent
      subjectivity that dual-annotator design exposes but cannot resolve.
    status: validated
    warmth: medium
    confidence: high
    source:
      - https://github.com/DDMAL/salami-data-public
      - https://ismir2011.ismir.net/papers/PS4-14.pdf
      - https://github.com/bmcfee/salami-data-public/tree/hierarchy-corrections
    tags: [dataset, external, audit, structure, segmentation]
    relates_to: [edm-drop-detection-paper]
    notes: >
      ANNOTATION METHODOLOGY: Nine trained annotators (named in CLIR report) following
      published annotator guide (Peeters & Deruty). Two independent annotators per track.
      Three parsed layers: (1) coarse/uppercase letters (A, B, C), (2) fine/lowercase
      (a, b, a'), (3) functional vocabulary (verse, chorus, bridge). Music from Codaich,
      Internet Archive, RWC, Isophonics — genre-diverse: classical, jazz, pop, world, live.

      REPRODUCIBILITY: Hard. No audio distribution. Must independently source from
      RWC (separate license/payment), Internet Archive (links may break), Codaich,
      and Isophonics. ~75% of non-public tracks matched to YouTube (matching-salami
      project) but this degrades over time. Multiple sources with different access
      methods make full reproduction a significant effort.

      STRENGTHS: Published methodology, trained annotators, dual annotation, hierarchical
      labels, broadest genre coverage, CC0 license, well-maintained with version history,
      standard benchmark for structure algorithms.

      WEAKNESSES: No audio distribution (the perennial SALAMI problem — must source
      independently, YouTube matching degrades). Pre-v2.0 formatting errors meant many
      papers trained on corrupt data; McFee's hierarchy-corrections fork needed for clean
      data but isn't the official repo. Internet Archive subset is live music (structurally
      different from studio). Low inter-annotator agreement is a property of the task but
      means single-annotator "ground truth" is unreliable.

  - id: giantsteps-tempo-key-audit
    date: 2026-02-21
    touched: 2026-02-21
    title: "GiantSteps Tempo + Key (664/604 tracks): independent audit"
    summary: >
      Two companion EDM datasets for tempo (664 tracks) and key (604 tracks) from
      Beatport previews. Annotations harvested from user forum corrections — crowdsourced,
      not expert-annotated. v1 tempo had ~8.9% error rate, substantially improved by
      Schreiber & Mueller's v2 (2018). Key dataset improved by Faraldo (GiantSteps+)
      with expert revision and confidence levels. 84.8% minor-key skew.
    status: validated
    warmth: medium
    confidence: high
    source:
      - https://github.com/GiantSteps/giantsteps-tempo-dataset
      - https://github.com/GiantSteps/giantsteps-key-dataset
      - https://ismir2018.ircam.fr/doc/pdfs/220_Paper.pdf
    tags: [dataset, external, audit, tempo, key, edm]
    relates_to: [edm-drop-detection-paper]
    notes: >
      ANNOTATION METHODOLOGY — TEMPO: v1 extracted from Beatport forum user corrections
      (crowdsourced, no formal protocol, no inter-annotator agreement). Schreiber &
      Mueller 2018 found v1 had ~8.9% incorrect annotations ignoring octave errors,
      2.5% "No Beat" segments. v2 used controlled crowdsourced experiment — considerably
      more reliable. 3 tracks still lack valid tempo in v2.

      ANNOTATION METHODOLOGY — KEY: Same Beatport forum provenance. GiantSteps+ revision
      (Faraldo 2017) substantially improved: 600 tracks with confidence levels, 500 with
      deep modal analysis. 84.8% minor-key skew (Fm, Cm, Gm most frequent) — reflects
      EDM conventions but creates class imbalance.

      REPRODUCIBILITY: Easy for now. Beatport 2-min MP3 previews downloadable via
      provided scripts (~1GB tempo, ~850MB key). Filenames match annotations directly.
      Risk: depends on Beatport URL patterns remaining stable long-term.

      STRENGTHS: EDM-specific (fills genuine gap). Audio downloadable (Beatport previews).
      Multiple formats. v2 tempo and GiantSteps+ key are meaningfully improved over v1.
      Genre annotations included.

      WEAKNESSES: v1 annotations unreliable — many older papers evaluated on these.
      Crowdsourced corrections have no formal protocol. 84.8% minor-key imbalance.
      2-min previews from high-energy sections (sampling bias). Octave errors fundamental
      issue in EDM tempo. Beatport URL stability not guaranteed long-term.

  - id: mir-dataset-landscape-gap
    date: 2026-02-21
    touched: 2026-02-21
    title: "No MIR dataset has both redistributable audio AND rigorous multi-annotator methodology"
    summary: >
      Cross-cutting finding from auditing 4 major MIR datasets: SALAMI has the best
      methodology but no audio. Harmonix has scale but single-annotator. GiantSteps has
      downloadable audio but crowdsourced provenance. Yadati has CC audio but under-documented
      expert labels. Every dataset requires understanding its specific failure modes before
      treating it as ground truth.
    status: validated
    warmth: medium
    confidence: high
    source: []
    tags: [dataset, external, audit, meta]
    relates_to: [yadati-edm-drops-audit, harmonix-set-audit, salami-dataset-audit, giantsteps-tempo-key-audit]
    notes: >
      This is the fundamental tradeoff in MIR dataset design: audio redistribution
      requires permissive licensing (CC), which limits the music pool. Rigorous annotation
      requires trained experts and multiple passes, which limits scale. No existing dataset
      solves both simultaneously. Researchers must either accept annotation quality
      limitations (Yadati, GiantSteps) or accept audio access friction (SALAMI, Harmonix).

      Reproduction ease ranks: Yadati (bundled on OSF, download and go) > GiantSteps
      (Beatport scripts, easy but URL-dependent) > Harmonix (YouTube fetch + DTW verify,
      degrades with link rot) > SALAMI (multiple external sources, some paywalled, major
      effort to assemble).

  - id: perceptual-spectrum-preprocessing
    date: 2026-02-21
    touched: 2026-02-21
    title: "Perceptual spectrum: equal-loudness weighting + frequency warping as early preprocessing"
    summary: >
      Apply two cheap transforms to raw FFT bins before any downstream analysis:
      (1) equal-loudness weighting (A/K-weight curve — per-bin multiply), (2) mel/Bark
      frequency warping. Everything downstream (band energies, MFCCs, contrast, derivatives)
      inherits perceptual grounding. No established name exists for this combined step.
    status: superseded
    warmth: high
    confidence: medium
    source: []
    tags: [feature-extraction, perception, architecture, feelings]
    relates_to: [continuous-perceptual-remap, excitation-pattern-future, signal-processing-tree, equal-loudness-weighting-dead-end]
    notes: >
      Two layers of human hearing that our pipeline currently handles unevenly:
      (1) Mel scale handles "pitch differences feel uneven" but (2) nothing handles
      "loudness feels uneven across frequencies" — bass needs more energy to sound
      equally loud as mids. A-weighting (ISO 226, 40-phon approximation) is the
      simplest correction. Currently our band energies over-represent bass relative
      to what the listener actually perceives. For LED mapping this means bass LEDs
      light up more than the music "feels" like they should.

  - id: continuous-perceptual-remap
    date: 2026-02-21
    touched: 2026-02-21
    title: "Continuous perceptual remapping vs discrete binning"
    summary: >
      Rather than binning FFT into mel/Bark bands then weighting, apply a single
      continuous remapping function directly to raw FFT bins. The mel scale and
      equal-loudness curves are both continuous functions — discretizing into bands
      early throws away resolution unnecessarily. Bin only at the final step
      (e.g. 5 LED zones), not at the analysis step.
    status: superseded
    warmth: high
    confidence: high
    source: []
    tags: [feature-extraction, perception, architecture]
    relates_to: [perceptual-spectrum-preprocessing, excitation-pattern-future]
    notes: >
      The literature bins first for historical reasons (Bark bands designed 1960s
      to match cochlear filter widths). With modern FFTs giving 512+ bins, there's
      no reason to throw away resolution early. The transform is just:
      perceived(f) = fft_energy(f) * loudness_weight(f), plotted on a mel-warped
      frequency axis. Defer binning to the last possible moment (LED zone mapping).
      This is the preferred direction for exploration.

  - id: excitation-pattern-future
    date: 2026-02-21
    touched: 2026-02-21
    title: "Excitation patterns (Zwicker/Moore) as future simplification via masking"
    summary: >
      Full psychoacoustic excitation patterns add masking (cross-frequency suppression)
      on top of loudness weighting + frequency warping. Masking removes sounds that
      are perceptually hidden by louder neighbors. This could further simplify the
      signal by removing content that isn't "top of mind" for the listener.
    status: spark
    warmth: medium
    confidence: medium
    source: []
    tags: [feature-extraction, perception, future]
    relates_to: [perceptual-spectrum-preprocessing, continuous-perceptual-remap]
    notes: >
      MP3/AAC use excitation patterns internally for compression — deciding what
      to throw away. The same logic applies to LEDs: if a sound is masked, the
      listener isn't tracking it, so maybe the LEDs shouldn't either. However,
      masking might suppress signals we DO want LEDs to react to (e.g. a quiet
      bass note under a loud kick). Worth revisiting after the simpler continuous
      remap is validated. Real-time feasible — MP3 encoders do it in real-time.
      Key terms: Zwicker model, Moore-Glasberg model, specific loudness, spreading
      function, Bark scale, excitation pattern.

  - id: signal-processing-tree
    date: 2026-02-21
    touched: 2026-02-21
    title: "Signal processing tree: data lineage of all computed features"
    summary: >
      All features derive from raw audio through a tree of transformations:
      decomposition (band split, MFCC, chroma), normalization (global, peak-decay, share),
      derivative (d/dt), second derivative (d²/dt²), integral (rolling sum),
      context statistics (z-score, ratio to median), similarity (Foote's matrix),
      peak/span detection. Most leaf nodes are 2-4 layers deep. Many combinations
      remain unexplored.
    status: validated
    warmth: medium
    confidence: high
    source: []
    tags: [architecture, feature-extraction, tools]
    relates_to: [derivatives-over-absolutes, perceptual-spectrum-preprocessing, continuous-perceptual-remap]
    notes: >
      Documented during session as a tree from raw waveform y through FFT/STFT,
      mel spectrogram, band energies, MFCCs, chroma, and all their derivatives/
      integrals/context-stats. Key insight: the tree is deep vertically (up to 4
      layers) but narrow horizontally (few source signals). Missing source branches
      include spectral flux (exists in effects code but not viewer) and crest factor.
      The perceptual spectrum preprocessing would add a new root transform that
      all branches inherit from.

  - id: onset-vs-absint-comparison
    date: 2026-02-21
    touched: 2026-02-21
    title: Onset strength is per-frame percussion; absint is stickier from 150ms window
    summary: >
      Side-by-side comparison (onset-absint lab variant) reveals: onset strength
      is sharper on attacks but blind to decay shape. AbsInt's 150ms integration
      window (~13 frames) smears transients into bumps but captures energy leaving.
      Harmonic vs percussive sounds have different "energy leaving" envelopes —
      absint sees this, onset doesn't.
    status: validated
    warmth: medium
    confidence: high
    source: web viewer onset-absint lab variant
    tags: [feature-extraction, beat-detection, comparison]
    relates_to: [absint-fails-percussive, onset-envelope-for-tempo, derivatives-over-absolutes]
    notes: >
      Onset strength = instantaneous spectral flux (good per-frame percussion detector).
      AbsInt = |d(RMS)/dt| summed over 150ms window (registers more activity, stickier).
      The difference signal |onset - absint| highlights where they disagree — mainly
      on sustained energy changes where absint sees the tail and onset doesn't.
      Practical implication: use onset strength for tight percussion triggering,
      absint for broader energy-change detection (builds, drops, swells).
      Hypothesis (untested): absint isolates percussion better when vocals are
      present — vocal spectral flux muddies onset strength, but absint's
      energy-leaving sensitivity sees through sustained vocal energy to the
      percussive transients underneath. Our calculus primitives (absint,
      derivatives, integrals) should be treated as custom features alongside
      standard librosa features in analysis tools, not just internal effect signals.

  - id: equal-loudness-weighting-dead-end
    date: 2026-02-21
    touched: 2026-02-21
    title: "Equal-loudness weighting on mastered music is counterproductive (dead end)"
    summary: >
      A/C/ISO 226 weighting applied to mixed music double-corrects — mix engineers
      already compensate for human hearing sensitivity. A-weighting (40 phon) eliminates
      sub-bass entirely. C-weighting is nearly flat (useless). ISO 226 at 100-115 phon
      over-attenuates bass. No phon level produces balanced band energy from mastered music.
    status: validated
    warmth: low
    confidence: high
    source: audio-reactive/tools/web_viewer.py
    tags: [feature-extraction, perception, dead-end]
    relates_to: [perceptual-spectrum-preprocessing, continuous-perceptual-remap, excitation-pattern-future]
    notes: >
      Three sub-findings from the implementation and debugging process:

      (1) ISO 226:2003 requires parametric computation (Eq 1 with alpha_f, L_U, T_f
      parameter tables) — lookup tables are error-prone. Our first attempt used hearing
      threshold T_f values as SPL curves, producing 8-20 dB errors at 1kHz where N phon
      must equal N dB SPL by definition.

      (2) Slaney mel filterbank normalization suppresses treble by 6.5x vs direct FFT
      bin summation because it divides each filter by bandwidth — designed for MFCCs,
      distorts energy ratios. For band energy computation in Hz-defined bands, direct
      FFT bin summation is more faithful.

      (3) The MFCC/mel pipeline handles pitch resolution (mel scale) and intensity
      compression (log) but NOT frequency-dependent loudness sensitivity — these are
      orthogonal perceptual axes. PLP (Hermansky 1990) includes equal-loudness
      pre-emphasis but is not standard in music analysis.

      Supersedes perceptual-spectrum-preprocessing and continuous-perceptual-remap.
      The real solution for balanced band display is per-band normalization against
      running history, not perceptual weighting of the spectrum.

  - id: per-band-normalization-with-dropout-handling
    date: 2026-02-21
    touched: 2026-02-21
    title: "Per-band normalization with asymmetric EMA and dropout detection for real-time LEDs"
    summary: >
      Normalize each band's energy against a sliding-window mean (not max) of its own
      history. Asymmetric EMA: instant attack (new max immediately adopted), constant
      decay (reference slowly drops so monotonic beats always register). Absolute
      threshold per band detects dropout; reference freezes during dropout so
      reintroduction after 15-30s produces massive normalized values — capturing the
      musical intention of EDM drops.
    status: exploring
    warmth: high
    confidence: medium
    source: audio-reactive/synchronicity-led/algorithms/per_band_normalization.md
    tags: [feature-extraction, effects, architecture, perception]
    relates_to: [equal-loudness-weighting-dead-end, rolling-integral-sustained-energy, airiness-context-deviation, accent-vs-groove-effects]
    notes: >
      Emerged from the equal-loudness dead end: the right approach isn't weighting
      the spectrum to match perception, it's normalizing each band against its own
      recent context. This mirrors the airiness finding — feelings are deviations
      from context, not absolute levels.

      Key design decisions (untested):
      (1) Sliding-window MEAN not max — tracks sustained energy level, not peak
      spikes. A single loud hit shouldn't reset the reference for minutes.
      (2) Asymmetric EMA — attack instant so new peaks are immediately visible,
      decay constant (not exponential) so the reference drifts down slowly. This
      means a steady kick drum always shows full brightness on every hit.
      (3) Absolute dropout threshold (not relative to current reference) — prevents
      false dropout detection in genres with sustained low-energy bands (e.g. techno
      kick patterns that are monotonic but not silent).
      (4) Reference freeze during dropout — when a band drops below absolute threshold,
      stop updating the reference. When energy returns after 15-30s, the frozen
      reference is much lower than the returning energy, so the entire reintroduction
      section "crashes hard" with huge normalized values until the reference catches up.
      This naturally captures the feel of EDM drops without explicit structure detection.

      Unresolved: exact absolute threshold values per band, decay rate, window size
      for the sliding mean. May need per-genre tuning or user-adjustable parameters.
      Could connect to the DAW concepts entry — the decay rate is essentially an
      ADSR release parameter.

  # ── Architecture Document (Feb 21) ─────────────────────────────────

  - id: genre-awareness-is-derivative
    date: 2026-02-21
    touched: 2026-02-21
    title: "Genre awareness is derivative of observable feature properties"
    summary: >
      Whether music is "accent" or "groove" comes from onset density, spectral
      continuity, and flourish ratio — not from a genre label. Genre is an emergent
      label on top of these properties, not a separate input axis. Excluded from the
      design space as an independent axis.
    status: validated
    warmth: medium
    confidence: high
    source: audio-reactive/synchronicity-led/ARCHITECTURE.md
    tags: [architecture, feature-extraction, philosophy]
    relates_to: [accent-vs-groove-effects, flourish-ratio, two-quality-axes]
    notes: >
      Reasoning: "rock" vs "electronic" tells you something about likely onset density
      and spectral profile, but you can measure those directly. A rock song with sparse
      percussion behaves like an electronic track for LED purposes. A rap track with
      heavy syncopation behaves like groove-oriented electronic. The features that matter
      (onset density, spectral continuity, flourish ratio) are directly observable without
      knowing the genre. Genre adds no information that the features don't already provide.
      Documented in ARCHITECTURE.md as the rationale for excluding genre from the 7-axis
      design space.

  - id: per-band-feature-expansion
    date: 2026-02-21
    touched: 2026-02-21
    title: "Per-band feature expansion multiplies useful feature space ~5x"
    summary: >
      Computing existing scalar features (absint, onset strength, RMS, spectral flux)
      independently per frequency band expands the feature space roughly 5-fold. Not
      all features benefit equally — chroma and MFCCs already decompose by frequency
      and gain nothing. Scalar energy/transient features gain the most.
    status: exploring
    warmth: medium
    confidence: medium
    source: audio-reactive/synchronicity-led/ARCHITECTURE.md
    tags: [feature-extraction, architecture, future]
    relates_to: [band-zone-pulse-effect, onset-vs-absint-comparison, signal-processing-tree]
    notes: >
      Already implemented for band_zone_pulse (per-band percussive peak detection)
      but not systematically explored. The question is whether per-band absint,
      per-band onset envelope, or per-band rolling integral reveal discriminative
      features that whole-signal versions miss. Standard bands: sub-bass (20-80Hz),
      bass (80-250Hz), low-mid (250-1kHz), high-mid (1-4kHz), treble (4-16kHz).

  - id: event-detection-viewer-only
    date: 2026-02-21
    touched: 2026-02-21
    title: "Event detection exists only in viewer.py (batch, non-streaming, disconnected)"
    summary: >
      All event detection code (dropout, re-entry, crescendo, climax) lives in
      viewer.py as batch-only visualization. No effect consumes it. Foote's
      checkerboard novelty requires an O(n^2) similarity matrix — fundamentally
      non-streaming. A streaming event detector needs a different approach.
    status: dead-end
    warmth: high
    confidence: high
    source: audio-reactive/tools/viewer.py
    tags: [architecture, effects, feature-extraction, future, dead-end]
    relates_to: [build-taxonomy, accent-vs-groove-effects, rolling-integral-sustained-energy]
    notes: >
      Architectural gap: the effects pipeline has no way to know about section-level
      events. Rolling integral slope (rising=build, falling=breakdown) is the most
      promising streaming substitute for Foote novelty. Per-band dropout detection
      (absolute threshold + reference freeze) is already designed but unimplemented
      (see per-band-normalization-with-dropout-handling). Fixing this is prerequisite
      for section-aware effects.
      UPDATE 2026-02-21: Removed Events A/B tabs from viewer — the computed event
      overlays (drops, risers, dropouts, harmonic spans) were too difficult to
      interpret and didn't effectively capture the event signals. The V:events
      toggle on the novelty panel remains as a placeholder for future work.

  - id: adaptation-time-constant-is-temporal-scope
    date: 2026-02-21
    touched: 2026-02-21
    title: "The adaptation time constant IS the temporal scope"
    summary: >
      To detect a phrase-level musical event, you need phrase-level memory.
      The adaptation time constant of a feature's running baseline determines
      what temporal scope of events it can detect. 150ms window → beat events.
      5s window → phrase events. Slow-decay peak → song-level adaptation.
    status: resonates
    warmth: high
    confidence: high
    source: audio-reactive/synchronicity-led/ARCHITECTURE.md
    tags: [architecture, feature-extraction, philosophy]
    relates_to: [airiness-context-deviation, per-band-normalization-with-dropout-handling, rolling-integral-sustained-energy]
    notes: >
      Unifies several findings: airiness-as-context-deviation (feelings are relative
      to running baseline), per-band normalization (adaptation rate shapes what events
      register), rolling integral (5s window for phrase-level detection). The insight
      is that these aren't separate design choices — they're all instances of the same
      principle: memory horizon defines detection capability.

  - id: context-expensive-code-cheap
    date: 2026-02-21
    touched: 2026-02-21
    title: "Context management is the expensive resource, not code"
    summary: >
      AI can write one-off effects cheaply. The expensive resource is context
      window — both the user's brain and Claude's tokens. Effects can all be
      one-offs and likely will need to be for ESP32 performance. No unified
      transformation framework needed. Investment goes into shared mental models
      (architecture docs, ledger) not code abstractions.
    status: resonates
    warmth: high
    confidence: high
    source: []
    tags: [architecture, methodology, philosophy]
    relates_to: [two-quality-axes, research-ledger-system]
    notes: >
      Two implications: (1) Don't invest in effect code reuse frameworks — each
      effect is cheap to write from scratch with the right context. (2) DO invest
      in context compression tools (this ledger, the architecture doc, MEMORY.md).
      The architecture document exists because of this insight — it's a context
      compression artifact, not a code design document.

  - id: vj-conventions-reuse-question
    date: 2026-02-21
    touched: 2026-02-21
    title: "Unresolved: adopt VJ conventions wholesale vs build custom features"
    summary: >
      Strategic fork: (a) adopt established VJ mapping conventions (Resolume,
      TouchDesigner, WLED) and focus effort on custom topology mapping, or
      (b) build custom audio features (calculus primitives, per-band expansion)
      that go beyond what VJ tools provide. Not mutually exclusive but affects
      where to invest effort.
    status: spark
    warmth: medium
    confidence: low
    source:
      - audio-reactive/research/landscape/RESEARCH_AUDIO_VISUAL_MAPPING.md
      - audio-reactive/synchronicity-led/library/VJ_AUDIO_VISUAL_MAPPING.md
    tags: [architecture, philosophy, future]
    relates_to: [two-quality-axes, wled-sr-reimplemented, fluora-pixelair-prior-art, zcr-redundant-with-fft]
    notes: >
      The two-quality-axes principle suggests this matters: VJ conventions are
      strong on mapping quality (proven visual patterns) even if decomposition
      quality is crude. Our calculus features may provide decomposition quality
      that VJ tools lack. The question is whether that decomposition advantage
      produces visibly better LEDs. Unresolved — needs A/B testing on hardware.
      VJ mapping conventions now documented in library (6 visual parameters,
      proven constants, perceptual science). No major VJ tool uses ZCR, MFCCs,
      or spectral rolloff — their pipelines are FFT bands + RMS + beat detection.

  - id: zcr-redundant-with-fft
    date: 2026-02-21
    touched: 2026-02-21
    title: "ZCR is redundant — spectral centroid is strictly better"
    summary: >
      Zero crossing rate correlates with spectral centroid at r=0.85 across
      8000 FMA Small tracks. ZCR is a cheap time-domain brightness proxy but
      adds nothing if FFT is already available. No major VJ software exposes
      ZCR. Genre separation exists but distributions overlap heavily.
    status: validated
    warmth: low
    confidence: high
    source: audio-reactive/synchronicity-led/library/VJ_AUDIO_VISUAL_MAPPING.md
    tags: [audio-features, dead-end]
    relates_to: [vj-conventions-reuse-question, two-quality-axes]
    notes: >
      Dataset-wide analysis (not anecdotal): computed ZCR stats for all 8000
      FMA Small tracks using precomputed features.csv. Genre ranking by median
      ZCR: Instrumental < Folk < Experimental < Pop < Electronic < International
      < Hip-Hop < Rock. Electronic is mid-pack (synth brightness + hi-hats offset
      bass), Rock highest (distortion = noise-like = high ZCR). ZCR-RMS correlation
      is weak — brightness and loudness are independent. Only use case would be
      on MCU without FFT capability, which doesn't apply to ESP32.

  - id: analysis-scripts-removed
    date: 2026-02-21
    touched: 2026-02-21
    title: "Removed analysis/scripts/ and output reports — consolidated to ledger + library"
    summary: >
      Deleted analysis/scripts/ (32 scripts, ~12K LOC), analysis/taste/ (13 reports),
      analysis/algorithms/ (11 reports), analysis/harmonix/ (9 reports). All conclusions
      already captured in ledger entries. Three reference docs moved to
      audio-reactive/synchronicity-led/library/ (see INDEX.md there).
    status: integrated
    warmth: low
    confidence: high
    source: git history (commit before deletion: b374d0b)
    tags: [repo-hygiene, tools]
    relates_to: [repo-discoverability]
    notes: >
      Everything recoverable from git history. Output report conclusions were
      redundant with ledger entries: taps-track-bass-peaks, airiness-context-deviation,
      flourish-ratio, flourish-audio-properties, derivatives-over-absolutes,
      build-taxonomy, beat-trackers-fail-dense-rock, bass-flux-electronic-failure,
      harmonix-set-audit. Three reference docs preserved in synchronicity-led/library/:
      LIBRARY_RESEARCH_SUMMARY.md, LED_MAPPING_GUIDE.md, HARMONIX_QUICKSTART.md.
